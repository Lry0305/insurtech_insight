from crawler.news_crawler import crawl_insurtech_news
from analysis.deepseek_analysis import extract_insight_from_article
from utils.save_results import save_results_to_json, save_results_to_csv
import json

# 1. çˆ¬å–æ–°é—»
df = crawl_insurtech_news(["ä¿é™©ç§‘æŠ€", "æ•°å­—ä¿é™©"], max_articles_per_keyword=10)

# 2. è°ƒç”¨å¤§æ¨¡å‹åˆ†æ
results = []
for i, row in df.iterrows():
    print(f"\nğŸ¯ æ­£åœ¨åˆ†ææ–‡ç« ï¼š{row['æ ‡é¢˜']}")
    output = extract_insight_from_article(row['æ­£æ–‡'])
    try:
        parsed = json.loads(output) if isinstance(output, str) and output.startswith("{") else {"åŸå§‹è¾“å‡º": output}
    except Exception:
        parsed = {"åŸå§‹è¾“å‡º": output}

    parsed["æ ‡é¢˜"] = row["æ ‡é¢˜"]
    parsed["æ­£æ–‡"] = row["æ­£æ–‡"]
    parsed["é“¾æ¥"] = row["é“¾æ¥"]
    results.append(parsed)

# 3. ä¿å­˜ç»“æœ
save_results_to_json(results, "insurtech_results.json")
save_results_to_csv(results, "insurtech_results.csv")