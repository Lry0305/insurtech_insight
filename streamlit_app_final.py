
import streamlit as st
import pandas as pd
import json
import plotly.express as px
from matplotlib import rcParams
import matplotlib.pyplot as plt
from collections import Counter
import os
from wordcloud import WordCloud
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import re

from agent.agent_api import query_industry_agent

rcParams['font.sans-serif'] = ['SimHei']

# é¡µé¢è®¾ç½®
st.set_page_config(page_title="ä¿é™©ç§‘æŠ€æ™ºèƒ½åˆ†æå¹³å°", layout="wide")
st.title("ğŸ“Š ä¿é™©ç§‘æŠ€è¡Œä¸šæ™ºèƒ½è§‚ç‚¹åˆ†æå¹³å°")

# åŠ è½½æ•°æ®
@st.cache_data
def load_data():
    return pd.read_csv("insurtech_results.csv")

df = load_data()
st.markdown(f"å…±åŠ è½½ **{len(df)}** æ¡æ–°é—»æ•°æ®")

# ========== JSON è§£æ ==========
def extract_json_fields(df):
    sentiments, opinions, keywords_all, subjects = [], [], [], []
    for raw in df["åŸå§‹è¾“å‡º"]:
        try:
            content = json.loads(raw.replace("```json", "").replace("```", ""))
            sentiments.append(content.get("æƒ…ç»ª", "æœªæå–"))
            opinions.append(content.get("è§‚ç‚¹", ""))
            kws = content.get("å…³é”®è¯", [])
            ents = content.get("ä¸»ä½“", [])
            if isinstance(kws, list): keywords_all.extend(kws)
            if isinstance(ents, list): subjects.append(ents)
        except:
            sentiments.append("è§£æå¤±è´¥")
            opinions.append("")
            subjects.append([])
    return sentiments, opinions, keywords_all, subjects

df["æƒ…ç»ª"], df["è§‚ç‚¹"], all_keywords, df["ä¸»ä½“"] = extract_json_fields(df)

# ========== æƒ…ç»ª & å…³é”®è¯åˆ†æ ==========
st.subheader("ğŸ“Š æƒ…ç»ª & å…³é”®è¯è¶‹åŠ¿æ€»è§ˆ")
sentiment_counts = df["æƒ…ç»ª"].value_counts().reset_index()
sentiment_counts.columns = ["æƒ…ç»ª", "æ•°é‡"]

kw_freq = Counter(all_keywords)
top_kw = kw_freq.most_common(20)
top_df = pd.DataFrame(top_kw, columns=["å…³é”®è¯", "å‡ºç°æ¬¡æ•°"])

col1, col2 = st.columns(2)
with col1:
    fig_sentiment = px.pie(sentiment_counts, names="æƒ…ç»ª", values="æ•°é‡", title="æƒ…ç»ªå æ¯”")
    st.plotly_chart(fig_sentiment, use_container_width=True)
with col2:
    fig_bar = px.bar(top_df, x="å…³é”®è¯", y="å‡ºç°æ¬¡æ•°", title="å…³é”®è¯å‡ºç°é¢‘ç‡", text="å‡ºç°æ¬¡æ•°")
    st.plotly_chart(fig_bar, use_container_width=True)

# ========== è¯äº‘ ==========
st.subheader("â˜ï¸ å…³é”®è¯è¯äº‘å›¾")
def get_chinese_font():
    possible_paths = [
        "C:/Windows/Fonts/simhei.ttf",
        "/System/Library/Fonts/STHeiti Medium.ttc",
        "/usr/share/fonts/truetype/arphic/ukai.ttc",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
    ]
    for path in possible_paths:
        if os.path.exists(path):
            return path
    return None

font_path = get_chinese_font()
if font_path:
    wordcloud = WordCloud(font_path=font_path, background_color="white", width=800, height=400)
    wordcloud.generate_from_frequencies(kw_freq)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation="bilinear")
    ax.axis("off")
    st.pyplot(fig)
else:
    st.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨ä¸­æ–‡å­—ä½“ï¼Œè¯äº‘å°†æ— æ³•æ˜¾ç¤ºä¸­æ–‡ã€‚")

# ========== èšç±»åˆ†æ ==========
st.subheader("ğŸ§  è§‚ç‚¹èšç±»åˆ†æ")
vectorizer = TfidfVectorizer(stop_words="english", max_features=100)
X = vectorizer.fit_transform(df["è§‚ç‚¹"].fillna(""))
k = 4
model = KMeans(n_clusters=k, random_state=42)
df["èšç±»æ ‡ç­¾"] = model.fit_predict(X)
cluster_counts = df["èšç±»æ ‡ç­¾"].value_counts().sort_index()
fig_cluster = px.bar(x=cluster_counts.index, y=cluster_counts.values,
                     labels={"x": "èšç±»æ ‡ç­¾", "y": "æ ·æœ¬æ•°"}, title="è§‚ç‚¹èšç±»åˆ†å¸ƒ")
st.plotly_chart(fig_cluster, use_container_width=True)

st.markdown("#### æ¯ç±»ä»£è¡¨æ€§è§‚ç‚¹ï¼ˆæ¯ç±»å‰3æ¡ï¼‰")
for i in range(k):
    st.markdown(f"**ğŸŒ€ èšç±» {i}ï¼š**")
    sample = df[df["èšç±»æ ‡ç­¾"] == i]["è§‚ç‚¹"].dropna().head(3).tolist()
    for text in sample:
        st.markdown(f"- {text}")

# ========== æ—¶é—´è¶‹åŠ¿åˆ†æ ==========
st.subheader("ğŸ“ˆ ä¿é™©ç§‘æŠ€æŠ¥é“éšæ—¶é—´å˜åŒ–")
def extract_date(source_info):
    match = re.search(r"(\d{4}-\d{2}-\d{2})", str(source_info))
    return match.group(1) if match else None

df["æ—¥æœŸ"] = df["æ¥æºä¿¡æ¯"].apply(extract_date)
timeline_df = df.dropna(subset=["æ—¥æœŸ"])
trend_data = timeline_df.groupby(["æ—¥æœŸ", "æƒ…ç»ª"]).size().reset_index(name="æ•°é‡")
fig_time = px.line(trend_data, x="æ—¥æœŸ", y="æ•°é‡", color="æƒ…ç»ª", markers=True,
                   title="ä¸åŒæƒ…ç»ªæŠ¥é“éšæ—¶é—´è¶‹åŠ¿")
st.plotly_chart(fig_time, use_container_width=True)

# ========== ä¸»ä½“å…¬å¸æƒ…ç»ªåˆ†æ ==========
st.subheader("ğŸ¢ ä¸»ä½“å…¬å¸æƒ…ç»ªé›·è¾¾å›¾")
def extract_entities(df):
    entities = []
    for raw in df["åŸå§‹è¾“å‡º"]:
        try:
            content = json.loads(raw.replace("```json", "").replace("```", ""))
            ents = content.get("ä¸»ä½“", [])
            if isinstance(ents, list):
                for e in ents:
                    entities.append((e, content.get("æƒ…ç»ª", "æœªæå–")))
        except:
            continue
    return pd.DataFrame(entities, columns=["ä¸»ä½“", "æƒ…ç»ª"])

ent_df = extract_entities(df)
if not ent_df.empty:
    pivot = ent_df.groupby(["ä¸»ä½“", "æƒ…ç»ª"]).size().unstack(fill_value=0)
    top_entities = pivot.sum(axis=1).sort_values(ascending=False).head(6).index
    radar = pivot.loc[top_entities]
    fig_radar = px.line_polar(radar.reset_index(), r=radar.sum(axis=1), theta=radar.index,
                              line_close=True, title="é«˜é¢‘å…¬å¸/æœºæ„æƒ…ç»ªå…³æ³¨å¼ºåº¦")
    st.plotly_chart(fig_radar, use_container_width=True)
else:
    st.info("æœªæˆåŠŸæå–å…¬å¸ä¸»ä½“")

# ========== æ™ºèƒ½ä½“æç¤º ========== 
st.subheader("ğŸ¤– è¡Œä¸šæ™ºèƒ½ä½“å»ºè®®ï¼ˆå®éªŒåŠŸèƒ½ï¼‰")
st.markdown("ä½ å¯ä»¥å°†èšç±»ç»“æœæˆ–å…³é”®è¯ä½œä¸ºè¾“å…¥æç¤ºè¯ï¼Œç”¨äº AI åˆ†æè¡Œä¸šæ€åŠ¿ï¼š")
st.code("è¯·åˆ†æå½“å‰ä¿é™©ç§‘æŠ€è¡Œä¸šçš„å…³é”®è¯ï¼šæ™ºèƒ½æ ¸ä¿ã€æ•°å­—é£æ§ã€å®¢æˆ·ä½“éªŒ")

# ========== å¯¼å‡ºæŠ¥å‘Š ==========
st.subheader("ğŸ“¤ ä¸€é”®å¯¼å‡ºæŠ¥å‘Š")
csv = df.to_csv(index=False).encode("utf-8-sig")
st.download_button("ğŸ“¥ ä¸‹è½½åˆ†æç»“æœï¼ˆCSVï¼‰", csv, file_name="insurtech_analysis.csv", mime="text/csv")

# ========== åŸå§‹æ•°æ® ==========
st.subheader("ğŸ“° åŸå§‹æ–°é—»æ•°æ®")
st.dataframe(df[["æ ‡é¢˜", "æƒ…ç»ª", "é“¾æ¥"]], use_container_width=True)

with st.chat_message("user"):
    user_question = st.text_input("ğŸ’¬ æé—®ä¿é™©ç§‘æŠ€ç›¸å…³é—®é¢˜", key="chat")

if user_question:
    with st.spinner("æ­£åœ¨åˆ†æï¼Œè¯·ç¨å€™..."):
        answer = query_industry_agent(user_question, api_key="sk-7b432ed6557d42939e8c52ae59a442c1")
        st.chat_message("ai").markdown(answer)
